{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bde213d0",
   "metadata": {},
   "source": [
    "# Base MedSAM2 Inference on SpineMetsCT Validation Set\n",
    "\n",
    "This notebook runs inference on the SpineMetsCT validation dataset using the base MedSAM2 model (without fine-tuning) and visualizes the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917bf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "# Add MedSAM2 to Python path\n",
    "sys.path.append(\"MedSAM2\")\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "# Ensure reproducibility\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch.manual_seed(2024)\n",
    "torch.cuda.manual_seed(2024)\n",
    "np.random.seed(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fb301d",
   "metadata": {},
   "source": [
    "## Define helper functions for loading data and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e29e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npz_data(npz_path):\n",
    "    \"\"\"Load data from an NPZ file.\"\"\"\n",
    "    data = np.load(npz_path)\n",
    "    # Updated keys based on the actual NPZ file structure\n",
    "    image = data['imgs']  # 3D volume with shape (8, 512, 512)\n",
    "    gt_mask = data['gts'] if 'gts' in data else None  # Ground truth mask\n",
    "    return image, gt_mask\n",
    "\n",
    "def convert_to_rgb(image):\n",
    "    \"\"\"Convert a grayscale image to RGB by repeating the channel 3 times.\n",
    "    \n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Grayscale image of shape (H, W).\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: RGB image of shape (3, H, W) in float32 format.\n",
    "    \"\"\"\n",
    "    # Normalize image to 0-255\n",
    "    if image.max() > 1.0:\n",
    "        image_norm = (image - image.min()) / (image.max() - image.min()) * 255.0\n",
    "    else:\n",
    "        image_norm = image * 255.0\n",
    "    \n",
    "    # Convert to uint8 for PIL\n",
    "    image_uint8 = image_norm.astype(np.uint8)\n",
    "    \n",
    "    # Convert to RGB using PIL\n",
    "    pil_image = Image.fromarray(image_uint8)\n",
    "    rgb_image = pil_image.convert('RGB')\n",
    "    \n",
    "    # Convert back to numpy array\n",
    "    rgb_array = np.array(rgb_image)\n",
    "    \n",
    "    # Rearrange to channels-first format (3, H, W) and convert to float32\n",
    "    rgb_array = np.transpose(rgb_array, (2, 0, 1)).astype(np.float32)\n",
    "    \n",
    "    # Normalize to 0-1 range for the model\n",
    "    rgb_array = rgb_array / 255.0\n",
    "    \n",
    "    return rgb_array\n",
    "\n",
    "def show_mask(mask, ax, mask_color=None, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Show mask overlay on the image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mask : numpy.ndarray\n",
    "        mask of the image\n",
    "    ax : matplotlib.axes.Axes\n",
    "        axes to plot the mask\n",
    "    mask_color : numpy.ndarray\n",
    "        color of the mask\n",
    "    alpha : float\n",
    "        transparency of the mask\n",
    "    \"\"\"\n",
    "    if mask_color is not None:\n",
    "        color = np.concatenate([mask_color, np.array([alpha])], axis=0)\n",
    "    else:\n",
    "        color = np.array([251/255, 252/255, 30/255, alpha])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def calculate_dice(pred_mask, gt_mask):\n",
    "    \"\"\"Calculate Dice coefficient between prediction and ground truth\"\"\"\n",
    "    intersection = (pred_mask * gt_mask).sum()\n",
    "    return (2. * intersection) / (pred_mask.sum() + gt_mask.sum() + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab5c2c",
   "metadata": {},
   "source": [
    "## Set up the model and prediction environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0837e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 1704 validation files\n",
      "Loading base MedSAM2 model...\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Path to validation data\n",
    "val_dir = \"processed_data/SpineMetsCT_npz/val\"\n",
    "\n",
    "# Get list of validation NPZ files\n",
    "val_files = glob.glob(os.path.join(val_dir, \"*.npz\"))\n",
    "print(f\"Found {len(val_files)} validation files\")\n",
    "\n",
    "# Load base model (MedSAM2 pretrained)\n",
    "print(\"Loading base MedSAM2 model...\")\n",
    "sam_checkpoint = \"MedSAM2/checkpoints/MedSAM2_2411.pt\"\n",
    "\n",
    "# Fix: Use absolute path to the config file\n",
    "config_file = \"MedSAM2/sam2/configs/sam2.1_hiera_t512.yaml\"\n",
    "\n",
    "# Build model and load weights\n",
    "sam = build_sam2(\n",
    "    config_file=config_file, \n",
    "    ckpt_path=sam_checkpoint,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Create predictor\n",
    "predictor = SAM2ImagePredictor(sam)\n",
    "\n",
    "# Create directory for saving results\n",
    "output_dir = \"inference_results_base_model\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9f211",
   "metadata": {},
   "source": [
    "## Test Data Loading and Preprocessing\n",
    "\n",
    "Let's first test our image loading and preprocessing to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7318b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image and check its properties\n",
    "if len(val_files) > 0:\n",
    "    sample_file = val_files[0]\n",
    "    print(f\"Sample file: {os.path.basename(sample_file)}\")\n",
    "    \n",
    "    # Load data\n",
    "    image_3d, gt_mask = load_npz_data(sample_file)\n",
    "    print(f\"Image shape: {image_3d.shape}, dtype: {image_3d.dtype}\")\n",
    "    print(f\"Mask shape: {gt_mask.shape}, dtype: {gt_mask.dtype}\")\n",
    "    print(f\"Image value range: [{image_3d.min()}, {image_3d.max()}]\")\n",
    "    \n",
    "    # Select a slice\n",
    "    slice_idx = image_3d.shape[0] // 2\n",
    "    image_slice = image_3d[slice_idx]\n",
    "    mask_slice = gt_mask[slice_idx]\n",
    "    \n",
    "    # Convert to RGB and check\n",
    "    rgb_image = convert_to_rgb(image_slice)\n",
    "    print(f\"RGB image shape: {rgb_image.shape}, dtype: {rgb_image.dtype}\")\n",
    "    print(f\"RGB image value range: [{rgb_image.min()}, {rgb_image.max()}]\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original grayscale image\n",
    "    axes[0].imshow(image_slice, cmap='gray')\n",
    "    axes[0].set_title(\"Original Grayscale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # RGB channels\n",
    "    rgb_display = np.transpose(rgb_image, (1, 2, 0))\n",
    "    axes[1].imshow(rgb_display)\n",
    "    axes[1].set_title(\"Converted RGB\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Ground truth mask\n",
    "    axes[2].imshow(image_slice, cmap='gray')\n",
    "    show_mask(mask_slice, ax=axes[2], mask_color=np.array([1.0, 0, 0]))\n",
    "    axes[2].set_title(\"Ground Truth Mask\")\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No validation files found to test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f4cca",
   "metadata": {},
   "source": [
    "## Run inference on validation dataset\n",
    "\n",
    "We'll process samples from the validation set and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 15067_12-03-2011-SpineSPINEBONESBRT Adult-72528_5.000000-SKINTOSKINSIM0.5MM15067a iMAR-28213_300.000000-Spine Segmentation-27808_144.npz...\n",
      "Loaded data shapes - Image: (8, 512, 512), Mask: (8, 512, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 240, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 277, in forward\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n               ~~~~~~~~~~~ <--- HERE\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/functional.py\", line 350, in normalize\n        raise TypeError(f\"img should be Tensor Image. Got {type(tensor)}\")\n\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ~~~~~~~~~~~~~ <--- HERE\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    if std.ndim == 1:\n        std = std.view(-1, 1, 1)\n    return tensor.sub_(mean).div_(std)\n           ~~~~~~~~~~~ <--- HERE\nRuntimeError: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m image_rgb \u001b[38;5;241m=\u001b[39m convert_to_rgb(image)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Set image for predictor\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Get automatic mask prediction\u001b[39;00m\n\u001b[1;32m     39\u001b[0m masks, scores, _ \u001b[38;5;241m=\u001b[39m predictor\u001b[38;5;241m.\u001b[39mpredict()\n",
      "File \u001b[0;32m/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp2/b10902078/MEDSAM/MedSAM2/sam2/sam2_image_predictor.py:112\u001b[0m, in \u001b[0;36mSAM2ImagePredictor.set_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage format not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m input_image \u001b[38;5;241m=\u001b[39m input_image[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mlen\u001b[39m(input_image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m input_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    117\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_image must be of size 1x3xHxW, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/tmp2/b10902078/MEDSAM/MedSAM2/sam2/utils/transforms.py:39\u001b[0m, in \u001b[0;36mSAM2Transforms.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(x)\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 240, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/transforms.py\", line 277, in forward\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n               ~~~~~~~~~~~ <--- HERE\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/functional.py\", line 350, in normalize\n        raise TypeError(f\"img should be Tensor Image. Got {type(tensor)}\")\n\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ~~~~~~~~~~~~~ <--- HERE\n  File \"/tmp2/b10902078/miniconda3/envs/MEDSAM/lib/python3.10/site-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n    if std.ndim == 1:\n        std = std.view(-1, 1, 1)\n    return tensor.sub_(mean).div_(std)\n           ~~~~~~~~~~~ <--- HERE\nRuntimeError: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "# Metrics dictionary\n",
    "metrics = {'dice': []}\n",
    "\n",
    "# Number of samples to process\n",
    "num_samples = 5\n",
    "max_slices_per_sample = 4  # Limit slices per sample to avoid too many visualizations\n",
    "\n",
    "# Run inference on validation set\n",
    "for i, npz_path in enumerate(tqdm(val_files[:num_samples])):\n",
    "    filename = os.path.basename(npz_path)\n",
    "    print(f\"\\nProcessing {filename}...\")\n",
    "    \n",
    "    # Load image and ground truth\n",
    "    image_3d, gt_mask = load_npz_data(npz_path)\n",
    "    print(f\"Loaded data shapes - Image: {image_3d.shape}, Mask: {gt_mask.shape}\")\n",
    "    \n",
    "    # Process each slice in the 3D volume (limit to a few slices for demonstration)\n",
    "    total_slices = image_3d.shape[0]\n",
    "    slice_indices = [\n",
    "        0,  # First slice\n",
    "        total_slices // 3,  # One-third through the volume\n",
    "        total_slices // 2,  # Middle slice\n",
    "        total_slices - 1,  # Last slice\n",
    "    ][:max_slices_per_sample]  # Limit to max_slices_per_sample\n",
    "    \n",
    "    for slice_idx in slice_indices:\n",
    "        image = image_3d[slice_idx]\n",
    "        \n",
    "        # Normalize image for visualization if needed\n",
    "        if image.max() > 1.0:\n",
    "            image_viz = (image - image.min()) / (image.max() - image.min())\n",
    "        else:\n",
    "            image_viz = image\n",
    "            \n",
    "        # Get ground truth slice\n",
    "        if gt_mask is not None:\n",
    "            gt_slice = gt_mask[slice_idx]\n",
    "        else:\n",
    "            gt_slice = None\n",
    "            \n",
    "        # Convert grayscale to RGB before passing to the model\n",
    "        image_rgb = convert_to_rgb(image)\n",
    "        \n",
    "        # Convert to PyTorch tensor and move to appropriate device\n",
    "        input_tensor = torch.tensor(image_rgb, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Set image for predictor\n",
    "        predictor.set_image(input_tensor)\n",
    "        \n",
    "        # Get automatic mask prediction\n",
    "        masks, scores, _ = predictor.predict()\n",
    "        \n",
    "        # Visualize and save results\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Plot original image\n",
    "        axes[0].imshow(image_viz, cmap='gray')\n",
    "        axes[0].set_title(\"Original Image\")\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Plot ground truth if available\n",
    "        if gt_slice is not None:\n",
    "            axes[1].imshow(image_viz, cmap='gray')\n",
    "            show_mask(gt_slice, ax=axes[1], mask_color=np.array([1.0, 0, 0]))\n",
    "            axes[1].set_title(\"Ground Truth\")\n",
    "            axes[1].axis('off')\n",
    "        else:\n",
    "            axes[1].set_visible(False)\n",
    "            \n",
    "        # Plot prediction\n",
    "        if len(masks) > 0:\n",
    "            # Use the highest scoring mask\n",
    "            mask = masks[0]  # Shape: H x W\n",
    "            axes[2].imshow(image_viz, cmap='gray')\n",
    "            show_mask(mask, ax=axes[2], mask_color=np.array([0, 0, 1.0]))\n",
    "            axes[2].set_title(f\"Prediction (Score: {scores[0]:.3f})\")\n",
    "            axes[2].axis('off')\n",
    "            \n",
    "            # Calculate Dice score if ground truth is available\n",
    "            if gt_slice is not None:\n",
    "                dice = calculate_dice(mask, gt_slice)\n",
    "                metrics['dice'].append(dice)\n",
    "                print(f\"Slice {slice_idx} - Dice score: {dice:.4f}\")\n",
    "        else:\n",
    "            axes[2].imshow(image_viz, cmap='gray')\n",
    "            axes[2].set_title(\"No prediction\")\n",
    "            axes[2].axis('off')\n",
    "        \n",
    "        # Save figure\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_slice{slice_idx}_results.png\"), dpi=150)\n",
    "        plt.close(fig)\n",
    "        \n",
    "    # Limit the number of files processed\n",
    "    if i >= num_samples - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346ddb81",
   "metadata": {},
   "source": [
    "## Analyze and display the quantitative results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a8a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average Dice score\n",
    "if metrics['dice']:\n",
    "    avg_dice = np.mean(metrics['dice'])\n",
    "    std_dice = np.std(metrics['dice'])\n",
    "    min_dice = np.min(metrics['dice'])\n",
    "    max_dice = np.max(metrics['dice'])\n",
    "    median_dice = np.median(metrics['dice'])\n",
    "    \n",
    "    print(f\"Dice statistics on {len(metrics['dice'])} validation slices:\")\n",
    "    print(f\"Average: {avg_dice:.4f}\")\n",
    "    print(f\"Standard deviation: {std_dice:.4f}\")\n",
    "    print(f\"Minimum: {min_dice:.4f}\")\n",
    "    print(f\"Maximum: {max_dice:.4f}\")\n",
    "    print(f\"Median: {median_dice:.4f}\")\n",
    "    \n",
    "    # Plot histogram of Dice scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(metrics['dice'], bins=10, edgecolor='black')\n",
    "    plt.axvline(avg_dice, color='red', linestyle='dashed', linewidth=1, label=f'Mean: {avg_dice:.4f}')\n",
    "    plt.axvline(median_dice, color='green', linestyle='dashed', linewidth=1, label=f'Median: {median_dice:.4f}')\n",
    "    plt.xlabel('Dice Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Dice Scores on Validation Set')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'dice_distribution.png'), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid samples for calculating Dice scores.\")\n",
    "    \n",
    "print(f\"Inference results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724b1b28",
   "metadata": {},
   "source": [
    "## 3D Volume Visualization\n",
    "\n",
    "Let's visualize a few representative slices from one 3D volume with their predictions overlaid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe894f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_visualize_volume(npz_path):\n",
    "    \"\"\"Process a whole 3D volume and visualize selected slices.\"\"\"\n",
    "    filename = os.path.basename(npz_path)\n",
    "    print(f\"\\nProcessing 3D volume: {filename}...\")\n",
    "    \n",
    "    # Load image and ground truth\n",
    "    image_3d, gt_mask = load_npz_data(npz_path)\n",
    "    print(f\"Volume shape: {image_3d.shape}\")\n",
    "    \n",
    "    # Create a 3D mask for predictions\n",
    "    pred_mask_3d = np.zeros_like(gt_mask)\n",
    "    \n",
    "    # Process all slices\n",
    "    slice_dice_scores = []\n",
    "    for slice_idx in tqdm(range(image_3d.shape[0]), desc=\"Processing slices\"):\n",
    "        image = image_3d[slice_idx]\n",
    "        \n",
    "        # Convert grayscale to RGB\n",
    "        image_rgb = convert_to_rgb(image)\n",
    "        \n",
    "        # Convert to PyTorch tensor and move to device\n",
    "        input_tensor = torch.tensor(image_rgb, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Set image for predictor\n",
    "        predictor.set_image(input_tensor)\n",
    "        \n",
    "        # Get automatic mask prediction\n",
    "        masks, scores, _ = predictor.predict()\n",
    "        \n",
    "        # Store prediction in 3D mask\n",
    "        if len(masks) > 0:\n",
    "            mask = masks[0]\n",
    "            pred_mask_3d[slice_idx] = mask\n",
    "            \n",
    "            # Calculate Dice score\n",
    "            if gt_mask is not None:\n",
    "                dice = calculate_dice(mask, gt_mask[slice_idx])\n",
    "                slice_dice_scores.append(dice)\n",
    "    \n",
    "    # Calculate volume-wide Dice score\n",
    "    volume_dice = calculate_dice(pred_mask_3d, gt_mask)\n",
    "    print(f\"Volume-wide Dice score: {volume_dice:.4f}\")\n",
    "    \n",
    "    # Select slices to visualize (start, 25%, 50%, 75%, end)\n",
    "    num_slices = image_3d.shape[0]\n",
    "    slice_indices = [\n",
    "        0,\n",
    "        num_slices // 4,\n",
    "        num_slices // 2,\n",
    "        3 * num_slices // 4,\n",
    "        num_slices - 1\n",
    "    ]\n",
    "    \n",
    "    # Visualize selected slices\n",
    "    fig, axes = plt.subplots(len(slice_indices), 3, figsize=(15, 4*len(slice_indices)))\n",
    "    \n",
    "    for i, slice_idx in enumerate(slice_indices):\n",
    "        # Prepare image for visualization\n",
    "        image = image_3d[slice_idx]\n",
    "        if image.max() > 1.0:\n",
    "            image_viz = (image - image.min()) / (image.max() - image.min())\n",
    "        else:\n",
    "            image_viz = image\n",
    "        \n",
    "        # Get masks\n",
    "        gt_slice = gt_mask[slice_idx]\n",
    "        pred_slice = pred_mask_3d[slice_idx]\n",
    "        \n",
    "        # Calculate Dice score for this slice\n",
    "        slice_dice = calculate_dice(pred_slice, gt_slice)\n",
    "        \n",
    "        # Original image\n",
    "        axes[i, 0].imshow(image_viz, cmap='gray')\n",
    "        axes[i, 0].set_title(f\"Slice {slice_idx}\")\n",
    "        axes[i, 0].axis('off')\n",
    "        \n",
    "        # Ground truth overlay\n",
    "        axes[i, 1].imshow(image_viz, cmap='gray')\n",
    "        show_mask(gt_slice, ax=axes[i, 1], mask_color=np.array([1.0, 0, 0]))\n",
    "        axes[i, 1].set_title(\"Ground Truth\")\n",
    "        axes[i, 1].axis('off')\n",
    "        \n",
    "        # Prediction overlay\n",
    "        axes[i, 2].imshow(image_viz, cmap='gray')\n",
    "        show_mask(pred_slice, ax=axes[i, 2], mask_color=np.array([0, 0, 1.0]))\n",
    "        axes[i, 2].set_title(f\"Prediction (Dice: {slice_dice:.3f})\")\n",
    "        axes[i, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}_3d_visualization.png\"), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    return volume_dice\n",
    "\n",
    "# Process a sample volume\n",
    "if len(val_files) > 0:\n",
    "    sample_volume = val_files[0]  # Use the first file\n",
    "    volume_dice = process_and_visualize_volume(sample_volume)\n",
    "else:\n",
    "    print(\"No validation files available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MEDSAM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
